[[3. Maximum likelihood and Bayesian estimation]]
	3.1 Introduction
	3.2 Maximum Likelihood Estimation
		3.2.1 The General Principle
		3.2.2 The Gaussian Case : Unknown $\mu$
		3.2.3 The Gaussian Case : Unknown $\mu$ and $\Sigma$ 
		3.2.4 Bias
	3.3 Bayesian Estimation
		3.3.1 The Class-Conditional Densities
		3.3.2 The Parameter Distribution
	3.4 Bayesian Parameter Estimation: Gaussian Case
		3.4.1 The Univariate Case: $p(\mu|D)$ 
		3.4.2 The Univariate Case: $p(x|D)$
		3.4.3 The Multivariate Case
	3.5 Bayesian Parameter Estimation: General
		Example 1: Recursive Bayes learning and maximum likelihood
		3.5.1 When do Maximum Lkielihood and Bayes methods differ?
		3.5.2 Non-informative Priors and Invariance
	3.6 Sufficient Statistics *
		Theorem 3.1: Factorization
		3.6.1 Sufficient Statistics and the Exponentian Family
	3.7 Problems of Dimensinality
		3.7.1 Accuracy, Dimension, and Training Sample Size
		3.7.2 Computational Complexity
		3.7.3 Overfitting
	3.8 Expectation-Maximization (EM)
		Algorithm 1: Expectation-Maximization
		Example 2: Expectation-Maximization for a 2D normal model
	3.9 Bayesian Belief Networks *
		Example 3: Belief networks for fish
	3.10 Hidden Markov Models *
		3.10.1 First-order Markov models
		3.10.2 First-order hidden Markov models
		3.10.3 Hidden Markov Model Computation
		3.10.4 Evaluation
		Algorithm 2: Forward
		Algorithm 3: Backward
		Example 4: Hidden Markov Model
		3.10.5 Decoding
		Algorithm 4: HMM decode
		Example 5: HMM decoding
		3.10.6 Learning
		Algorithm 5: Forward-Backward